# ğŸ›’ E-Commerce Data Lakehouse Pipeline  

This project demonstrates an **end-to-end data lakehouse solution** for e-commerce data, leveraging **Azure Data Lake Gen2, Azure Data Factory (ADF), and Azure Databricks (PySpark)**.  
It follows the **Medallion Architecture (Bronze, Silver, Gold layers)** to enable **scalable, reliable, and high-quality analytics**.  

---

## ğŸš€ Project Workflow  

1. **Data Ingestion**  
   - Raw CSVs (users, buyers, sellers, countries) ingested into Azure Data Lake Gen2 (landing zone).  
   - Automated **ADF pipelines** convert CSVs to Parquet and move them into the **Bronze layer**.  
   - Event-based triggers run pipelines automatically when new files arrive.  

2. **Data Transformation**  
   - **Bronze Layer**: Raw Parquet files stored with minimal processing, ensuring traceability and schema evolution.  
   - **Silver Layer**: Data cleaning (deduplication, type casting, missing values), standardization, and enrichment using PySpark in Databricks.  
   - **Gold Layer**: Curated Delta tables created with joins, aggregations, and business logic for analytics-ready consumption.  

3. **Data Consumption**  
   - Gold tables queried in **Databricks SQL**.  
   - Visualized with **Tableau/Power BI** dashboards for KPIs such as sales, user activity, and regional performance.  

---

## ğŸ› ï¸ Tech Stack  

- **Azure Data Factory** â†’ Data ingestion pipelines  
- **Azure Data Lake Gen2** â†’ Data storage (Bronze, Silver, Gold layers)  
- **Azure Databricks (PySpark)** â†’ Data transformation and enrichment  
- **Delta Lake** â†’ Unified lakehouse with schema evolution & ACID transactions  
- **BI Tools** â†’ Tableau / Power BI for dashboarding  

---

## ğŸ“‚ Repository Structure  

```bash
ecommerce-data-lakehouse/
â”‚
â”œâ”€â”€ datasets/                 # Sample e-commerce raw datasets (CSV)
â”‚
â”œâ”€â”€ pipelines/                # Azure Data Factory pipeline JSONs
â”‚   â”œâ”€â”€ users_pipeline.json
â”‚   â””â”€â”€ transactions_pipeline.json
â”‚
â”œâ”€â”€ notebooks/                # Azure Databricks notebooks (PySpark transformations)
â”‚   â”œâ”€â”€ bronze_to_silver.ipynb
â”‚   â”œâ”€â”€ silver_to_gold.ipynb
â”‚   â””â”€â”€ kpi_aggregations.ipynb
â”‚
â”œâ”€â”€ docs/                     # Architecture & documentation
â”‚   â”œâ”€â”€ data_architecture.png
â”‚   â”œâ”€â”€ medallion_layers.md
â”‚   â””â”€â”€ business_kpis.md
â”‚
â””â”€â”€ README.md                 # Project overview

```
---

## ğŸ¯ Key Skills Demonstrated  

- Building a **Lakehouse Architecture** on Azure (ADF, ADLS, Databricks)  
- Implementing **Medallion architecture** (Bronze â†’ Silver â†’ Gold)  
- Writing **PySpark transformations** for cleaning, enrichment, and aggregation  
- Delivering **BI-ready curated data** for dashboards and analytics  
- Showcasing **cloud-native scalable pipelines** for real-world e-commerce use cases  

---

## ğŸ“ˆ Example Insights  

- Top-selling product categories by revenue and geography  
- Buyer-seller transaction patterns and customer activity trends  
- Regional sales distribution and user engagement over time  

---

